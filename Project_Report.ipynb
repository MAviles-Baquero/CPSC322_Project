{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19's Impact on Healthcare Accessibility\n",
    "By: Tristan Call and Maria Elena Aviles-Baquero  \n",
    "CPSC 322, Spring 2021  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Database:\n",
    "This section must briefly describe the dataset you used and the classification task you implemented (e.g., what were you trying to classify in the dataset).\n",
    "\n",
    "We utilized week 21 of the Household Pulse Survey Public Use File, which covered the time period from December 9 – December 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Findings:\n",
    "You should also briefly describe your findings (e.g., what classifier approach performed the best).  \n",
    "Overall we discovered that a random forest classifier was our best classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database information\n",
    "Information about the dataset itself, e.g., the attributes and attribute types, the number of instances, and the attribute being used as the label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "# uncomment once you paste your mypytable.py into mysklearn package\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.plot_utils\n",
    "importlib.reload(mysklearn.plot_utils)\n",
    "import mysklearn.plot_utils as plot_utils\n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MySimpleLinearRegressor, MyNaiveBayesClassifier, MyDecisionTreeClassifier, MyZeroRClassifier, MyRandomClassifier, MyRandomForestClassifier\n",
    "\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulate Data into Useable Format\n",
    "The first thing we need to do is grab the data from the sas file and manipulate it into a format and size which is workable with our very much not optimized dataset. Part of this involves dropping rows with NaNs or -99s (seen but unanswered questions) in them ahead of time. Overall we aim to go from about 70,000 results to a more reasonable < 10,000 so that our computers can run it in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TBIRTH_YEAR  EGENDER  RHISPANIC  RRACE  EEDUC  INCOME  DELAY  NOTGET\n",
      "1          1969.0      2.0        1.0    1.0    7.0     6.0    1.0     2.0\n",
      "2          1959.0      2.0        1.0    1.0    7.0     4.0    1.0     1.0\n",
      "4          1967.0      1.0        1.0    1.0    4.0     6.0    2.0     2.0\n",
      "5          1965.0      1.0        1.0    1.0    7.0     6.0    2.0     2.0\n",
      "6          1962.0      2.0        1.0    2.0    4.0     1.0    2.0     2.0\n",
      "...           ...      ...        ...    ...    ...     ...    ...     ...\n",
      "4993       1964.0      2.0        1.0    1.0    4.0     1.0    2.0     2.0\n",
      "4994       1984.0      1.0        1.0    1.0    4.0     7.0    1.0     1.0\n",
      "4995       1973.0      1.0        1.0    1.0    6.0     8.0    2.0     2.0\n",
      "4997       1976.0      2.0        1.0    1.0    3.0     3.0    1.0     1.0\n",
      "4999       1958.0      2.0        1.0    1.0    7.0     5.0    2.0     2.0\n",
      "\n",
      "[3909 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Grab the data\n",
    "week21_filename = os.path.join(\"input_data\", \"pulse2020_puf_21.sas7bdat\")\n",
    "iterator = pd.read_sas(week21_filename, chunksize=5000)\n",
    "alldata = []\n",
    "for chunk in iterator:\n",
    "    alldata.append(chunk)\n",
    "\n",
    "relevant_attributes = [\"TBIRTH_YEAR\", \"EGENDER\", \"RHISPANIC\", \"RRACE\", \"EEDUC\", \"INCOME\", \"DELAY\", \"NOTGET\"]\n",
    "\n",
    "# Grab a chunk of data with the attributes we are interested in, minus Nans, and save to a local file\n",
    "data = alldata[0][[\"TBIRTH_YEAR\", \"EGENDER\", \"RHISPANIC\", \"RRACE\", \"EEDUC\", \"INCOME\", \"DELAY\", \"NOTGET\"]]\n",
    "working_data_filename = os.path.join(\"input_data\", \"week21_working.csv\")\n",
    "nafree_data = data.dropna()\n",
    "\n",
    "# Get rid of -99 results (aka seen but not answered)\n",
    "nafree_data = nafree_data[nafree_data.INCOME != -99]\n",
    "nafree_data = nafree_data[nafree_data.DELAY != -99]\n",
    "nafree_data = nafree_data[nafree_data.NOTGET != -99]\n",
    "print(nafree_data)\n",
    "\n",
    "# Save to file\n",
    "nafree_data.to_csv(working_data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize the data\n",
    "Next we want to get the data into a more useful format. Step one of this is chunk years into decades to have a reasonable number of attribute values for year according to the below:\n",
    "\n",
    "years | label\n",
    "-|-\n",
    "1932-1941 | 1\n",
    "1942-1951 | 2\n",
    "1952-1961 | 3\n",
    "1962-1971 | 4\n",
    "1972-1981 | 5\n",
    "1982-1991 | 6\n",
    "1992-2002 | 7\n",
    "\n",
    "Next we want to create a DELAYNOTGET column as a composite of delay and notget so we can look into both these attributes at ounce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "# Load the data into a mypytable for future analysis\n",
    "overall_table = MyPyTable()\n",
    "overall_table.load_from_file(working_data_filename)\n",
    "overall_table.convert_to_numeric()\n",
    "\n",
    "# Convert year into bigger categorical chunks\n",
    "year_col = overall_table.get_column(\"TBIRTH_YEAR\")\n",
    "year_label = [x + 1 for x in range(7)]\n",
    "cutoffs = [1932 + 10 * x for x in range(8)]\n",
    "year_col = myutils.categorize_continuous_list(year_col, cutoffs, year_label)\n",
    "\n",
    "# Create DELAYNOTGET column\n",
    "delay = overall_table.get_column(\"DELAY\")\n",
    "notget = overall_table.get_column(\"NOTGET\")\n",
    "delaynotget = []\n",
    "for i in range(len(delay)):\n",
    "    if delay[i] == 1 or notget[i] == 1:\n",
    "        delaynotget.append(1)\n",
    "    else:\n",
    "        delaynotget.append(2)\n",
    "        \n",
    "# Combine all the above into the overall_table\n",
    "overall_table.column_names.append(\"DELAYNOTGET\")\n",
    "overall_table.data = [[overall_table.data[i][0]] + [year_col[i]] + overall_table.data[i][2:] + [delaynotget[i]] for i in range(len(year_col))]\n",
    "#overall_table.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "Relevant summary statistics about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribute      min    max    mid      avg    median       std\n",
      "-----------  -----  -----  -----  -------  --------  --------\n",
      "TBIRTH_YEAR      1      7    4    4.0967          4  1.55314\n",
      "EGENDER          1      2    1.5  1.59964         2  0.489971\n",
      "RHISPANIC        1      2    1.5  1.08314         1  0.276096\n",
      "RRACE            1      4    2.5  1.31491         1  0.778868\n",
      "EEDUC            1      7    4    5.33078         6  1.42407\n",
      "INCOME           1      8    4.5  4.5559          5  2.07749\n",
      "DELAY            1      2    1.5  1.65004         2  0.476958\n",
      "NOTGET           1      2    1.5  1.74648         2  0.435025\n",
      "DELAYNOTGET      1      2    1.5  1.61474         2  0.486658\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "importlib.reload(mysklearn.plot_utils)\n",
    "import mysklearn.plot_utils as plot_utils\n",
    "\n",
    "# use overall_table object declared above to compute the stats for all attributes\n",
    "table_stats = overall_table.compute_summary_statistics(overall_table.column_names[1:])\n",
    "# print out the statistics table\n",
    "table_stats.pretty_print()\n",
    "\n",
    "items, values = myutils.get_item_frequency(overall_table.get_column(\"DELAYNOTGET\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualizations\n",
    "Data visualizations highlighting important/interesting aspects of your dataset. Visualizations may include frequency distributions, comparisons of attributes (scatterplot, multiple frequency diagrams), box and whisker plots, etc. The goal is not to include all possible diagrams, but instead to select and highlight diagrams that provide insight about the dataset itself.\n",
    "Note that this section must describe the above (in paragraph form) and not just provide diagrams and statistics. Also, each figure included must have a figure caption (Figure number and textual description) that is referenced from the text (e.g., “Figure 2 shows a frequency diagram for ...”).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "# get subtables group by whether or not an instance was delayed/didn't get care or did receive care without delay\n",
    "group_names, subtables = overall_table.group_by(\"DELAYNOTGET\")\n",
    "# first subtable represents the instances where the individual got delayed care or did not get any\n",
    "delayed_or_none = MyPyTable(overall_table.column_names, subtables[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "First we will break the information into the appropriate format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break information into X_train and class_label\n",
    "X_train = overall_table.get_columns([\"TBIRTH_YEAR\", \"EGENDER\", \"RHISPANIC\", \"RRACE\", \"EEDUC\", \"INCOME\"])\n",
    "X_train = X_train.data\n",
    "Y_train = overall_table.get_column(\"DELAYNOTGET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process\n",
    "To find the best classifier, we first computed 2 baseline classifiers, ZeroR and random. We then tested naive bayes, decision tree, and random forest classifiers and looked at how they compared to the baselines and each other. All of these classifiers were loosley based on the sklearn implementation, but featured simpler algorithms made by the authors and less optimization (https://scikit-learn.org/stable/).\n",
    "\n",
    "### Evaluation\n",
    "To determine which classifier was the best, we ran each of them over a stratified 10-fold cross validation testing technique for accuracy. The exception was the random forest classifier, which required its own unique approach. We then plugged these results into a confusion matrix to determine if the classifier was better at one or another prediction. Given roughly equal accuracy (within about 1%), the classifier where the recognition rates of all class labels were closer to being the same won out. Meaning any classifier which at least matched the zero R classifier in accuracy, but didn't have a 0%, 100% split in recognition rates, would win out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Baseline\n",
    "First we will compute the baseline classifiers to get an idea of how must we must improve our classifiers.\n",
    "### Zero R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Predictive Accuracy\n",
      "===========================================\n",
      "Stratified 10-Fold Cross Validation\n",
      "Zero R: accuracy = 0.614735226400614, error rate = 0.38526477359938605\n"
     ]
    }
   ],
   "source": [
    "print('''===========================================\n",
    "Predictive Accuracy\n",
    "===========================================\n",
    "Stratified 10-Fold Cross Validation''')\n",
    "k = 10\n",
    "all_predicted_delay_zero = []\n",
    "all_actual_delay_zero = []\n",
    "\n",
    "# Get training data\n",
    "train_folds, test_folds = myevaluation.stratified_kfold_cross_validation(X_train, Y_train, k)\n",
    "for i in range(k):\n",
    "    # Sort training data\n",
    "    xtrain = myutils.distribute_data_by_index(X_train, train_folds[i])\n",
    "    ytrain = myutils.distribute_data_by_index(Y_train, train_folds[i])\n",
    "    xtest = myutils.distribute_data_by_index(X_train, test_folds[i])\n",
    "    ytest = myutils.distribute_data_by_index(Y_train, test_folds[i])\n",
    "\n",
    "    # Compute prediction and convert\n",
    "    zero = MyZeroRClassifier()\n",
    "    zero.fit(xtrain, ytrain)\n",
    "    predicted_delay = zero.predict(xtest)\n",
    "    all_predicted_delay_zero += predicted_delay\n",
    "    all_actual_delay_zero += ytest\n",
    "    \n",
    "# Calculate overall accuracy\n",
    "accuracy = myutils.calculate_accuracy(all_predicted_delay_zero, all_actual_delay_zero)\n",
    "error_rate = 1- accuracy\n",
    "\n",
    "print(\"Zero R: accuracy = \" + str(accuracy) + \", error rate = \" + str(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Confusion Matrices\n",
      "===========================================\n",
      "Zero R (Stratified 10-Fold Cross Validation):\n",
      "==================  ==================  =============  =======  =================\n",
      "Delayed/canceled      Delayed/canceled    Not delayed    Total    Recognition (%)\n",
      "==================  ==================  =============  =======  =================\n",
      "Delayed/canceled                     0           1506     1506                  0\n",
      "Not delayed                          0           2403     2403                100\n",
      "==================  ==================  =============  =======  =================\n"
     ]
    }
   ],
   "source": [
    "print('''===========================================\n",
    "Confusion Matrices\n",
    "===========================================\n",
    "Zero R (Stratified 10-Fold Cross Validation):''')\n",
    "\n",
    "ylabels = list(set(Y_train))\n",
    "matrix = myevaluation.confusion_matrix(all_actual_delay_zero, all_predicted_delay_zero, ylabels)\n",
    "header = myutils.format_confusion_matrix_into_table(matrix, [\"Delayed/canceled\", \"Not delayed\"], \"Delayed/canceled\")\n",
    "\n",
    "print(tabulate(matrix, headers=header, tablefmt=\"rst\", numalign=\"right\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Predictive Accuracy\n",
      "===========================================\n",
      "Stratified 10-Fold Cross Validation\n",
      "Random: accuracy = 0.5305704783832182, error rate = 0.46942952161678175\n"
     ]
    }
   ],
   "source": [
    "print('''===========================================\n",
    "Predictive Accuracy\n",
    "===========================================\n",
    "Stratified 10-Fold Cross Validation''')\n",
    "k = 10\n",
    "all_predicted_delay_random = []\n",
    "all_actual_delay_random = []\n",
    "\n",
    "# Get training data\n",
    "train_folds, test_folds = myevaluation.stratified_kfold_cross_validation(X_train, Y_train, k)\n",
    "for i in range(k):\n",
    "    # Sort training data\n",
    "    xtrain = myutils.distribute_data_by_index(X_train, train_folds[i])\n",
    "    ytrain = myutils.distribute_data_by_index(Y_train, train_folds[i])\n",
    "    xtest = myutils.distribute_data_by_index(X_train, test_folds[i])\n",
    "    ytest = myutils.distribute_data_by_index(Y_train, test_folds[i])\n",
    "\n",
    "    # Compute prediction and convert\n",
    "    random = MyRandomClassifier()\n",
    "    random.fit(xtrain, ytrain)\n",
    "    predicted_delay = random.predict(xtest)\n",
    "    all_predicted_delay_random += predicted_delay\n",
    "    all_actual_delay_random += ytest\n",
    "    \n",
    "# Calculate overall accuracy\n",
    "accuracy = myutils.calculate_accuracy(all_predicted_delay_random, all_actual_delay_random)\n",
    "error_rate = 1- accuracy\n",
    "\n",
    "print(\"Random: accuracy = \" + str(accuracy) + \", error rate = \" + str(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Confusion Matrices\n",
      "===========================================\n",
      "Random (Stratified 10-Fold Cross Validation):\n",
      "==================  ==================  =============  =======  =================\n",
      "Delayed/canceled      Delayed/canceled    Not delayed    Total    Recognition (%)\n",
      "==================  ==================  =============  =======  =================\n",
      "Delayed/canceled                   581            925     1506              38.58\n",
      "Not delayed                        910           1493     2403              62.13\n",
      "==================  ==================  =============  =======  =================\n"
     ]
    }
   ],
   "source": [
    "print('''===========================================\n",
    "Confusion Matrices\n",
    "===========================================\n",
    "Random (Stratified 10-Fold Cross Validation):''')\n",
    "\n",
    "ylabels = list(set(Y_train))\n",
    "matrix = myevaluation.confusion_matrix(all_actual_delay_random, all_predicted_delay_random, ylabels)\n",
    "header = myutils.format_confusion_matrix_into_table(matrix, [\"Delayed/canceled\", \"Not delayed\"], \"Delayed/canceled\")\n",
    "\n",
    "print(tabulate(matrix, headers=header, tablefmt=\"rst\", numalign=\"right\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Actual Classifiers\n",
    "Next we try running naive bayes, decision tree, and random forest classifiers over our database and see how they compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Predictive Accuracy\n",
      "===========================================\n",
      "Stratified 10-Fold Cross Validation\n",
      "Naive bayes: accuracy = 0.6144794064978255, error rate = 0.3855205935021745\n"
     ]
    }
   ],
   "source": [
    "print('''===========================================\n",
    "Predictive Accuracy\n",
    "===========================================\n",
    "Stratified 10-Fold Cross Validation''')\n",
    "k = 10\n",
    "all_predicted_delay_bayes = []\n",
    "all_actual_delay_bayes = []\n",
    "\n",
    "# Get training data\n",
    "train_folds, test_folds = myevaluation.stratified_kfold_cross_validation(X_train, Y_train, k)\n",
    "for i in range(k):\n",
    "    # Sort training data\n",
    "    xtrain = myutils.distribute_data_by_index(X_train, train_folds[i])\n",
    "    ytrain = myutils.distribute_data_by_index(Y_train, train_folds[i])\n",
    "    xtest = myutils.distribute_data_by_index(X_train, test_folds[i])\n",
    "    ytest = myutils.distribute_data_by_index(Y_train, test_folds[i])\n",
    "\n",
    "    # Compute prediction and convert\n",
    "    bayes = MyNaiveBayesClassifier()\n",
    "    bayes.fit(xtrain, ytrain)\n",
    "    predicted_delay = bayes.predict(xtest)\n",
    "    all_predicted_delay_bayes += predicted_delay\n",
    "    all_actual_delay_bayes += ytest\n",
    "    \n",
    "# Calculate overall accuracy\n",
    "accuracy = myutils.calculate_accuracy(all_predicted_delay_bayes, all_actual_delay_bayes)\n",
    "error_rate = 1- accuracy\n",
    "\n",
    "print(\"Naive bayes: accuracy = \" + str(accuracy) + \", error rate = \" + str(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Confusion Matrices\n",
      "===========================================\n",
      "Naive bayes (Stratified 10-Fold Cross Validation):\n",
      "==================  ==================  =============  =======  =================\n",
      "Delayed/canceled      Delayed/canceled    Not delayed    Total    Recognition (%)\n",
      "==================  ==================  =============  =======  =================\n",
      "Delayed/canceled                   221           1285     1506              14.67\n",
      "Not delayed                        222           2181     2403              90.76\n",
      "==================  ==================  =============  =======  =================\n"
     ]
    }
   ],
   "source": [
    "print('''===========================================\n",
    "Confusion Matrices\n",
    "===========================================\n",
    "Naive bayes (Stratified 10-Fold Cross Validation):''')\n",
    "\n",
    "ylabels = list(set(Y_train))\n",
    "matrix = myevaluation.confusion_matrix(all_actual_delay_bayes, all_predicted_delay_bayes, ylabels)\n",
    "header = myutils.format_confusion_matrix_into_table(matrix, [\"Delayed/canceled\", \"Not delayed\"], \"Delayed/canceled\")\n",
    "\n",
    "print(tabulate(matrix, headers=header, tablefmt=\"rst\", numalign=\"right\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes had comparable accuracy to the Zero R classifier, as well as better delayed/canceled recognition. It also has better accuracy, and worse recognition than the random classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Predictive Accuracy\n",
      "===========================================\n",
      "Stratified 10-Fold Cross Validation\n",
      "Decision Tree: accuracy = 0.6091071885392684, error rate = 0.3908928114607316\n"
     ]
    }
   ],
   "source": [
    "print('''===========================================\n",
    "Predictive Accuracy\n",
    "===========================================\n",
    "Stratified 10-Fold Cross Validation''')\n",
    "k = 10\n",
    "all_predicted_delay_tree = []\n",
    "all_actual_delay_tree = []\n",
    "\n",
    "# Get training data\n",
    "train_folds, test_folds = myevaluation.stratified_kfold_cross_validation(X_train, Y_train, k)\n",
    "for i in range(k):\n",
    "    # Sort training data\n",
    "    xtrain = myutils.distribute_data_by_index(X_train, train_folds[i])\n",
    "    ytrain = myutils.distribute_data_by_index(Y_train, train_folds[i])\n",
    "    xtest = myutils.distribute_data_by_index(X_train, test_folds[i])\n",
    "    ytest = myutils.distribute_data_by_index(Y_train, test_folds[i])\n",
    "\n",
    "    # Compute prediction and convert\n",
    "    tree = MyDecisionTreeClassifier()\n",
    "    tree.fit(xtrain, ytrain, ['RHISPANIC', 'RRACE', 'TBIRTH_YEAR', 'INCOME', 'label'])\n",
    "    predicted_delay = tree.predict(xtest)\n",
    "    all_predicted_delay_tree += predicted_delay\n",
    "    all_actual_delay_tree += ytest\n",
    "# Calculate overall accuracy\n",
    "accuracy = myutils.calculate_accuracy(all_predicted_delay_tree, all_actual_delay_tree)\n",
    "error_rate = 1- accuracy\n",
    "\n",
    "print(\"Decision Tree: accuracy = \" + str(accuracy) + \", error rate = \" + str(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Confusion Matrices\n",
      "===========================================\n",
      "Decision Tree (Stratified 10-Fold Cross Validation):\n",
      "==================  ==================  =============  =======  =================\n",
      "Delayed/canceled      Delayed/canceled    Not delayed    Total    Recognition (%)\n",
      "==================  ==================  =============  =======  =================\n",
      "Delayed/canceled                   157           1349     1506              10.42\n",
      "Not delayed                        179           2224     2403              92.55\n",
      "==================  ==================  =============  =======  =================\n"
     ]
    }
   ],
   "source": [
    "print('''===========================================\n",
    "Confusion Matrices\n",
    "===========================================\n",
    "Decision Tree (Stratified 10-Fold Cross Validation):''')\n",
    "\n",
    "ylabels = list(set(Y_train))\n",
    "matrix = myevaluation.confusion_matrix(all_actual_delay_tree, all_predicted_delay_tree, ylabels)\n",
    "header = myutils.format_confusion_matrix_into_table(matrix, [\"Delayed/canceled\", \"Not delayed\"], \"Delayed/canceled\")\n",
    "\n",
    "print(tabulate(matrix, headers=header, tablefmt=\"rst\", numalign=\"right\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree, as you can see, did not have very good performance. It was out performed slightly by the zero R and Naive Bayes classifiers in terms of accuracy only had the benefit of better delayed/canceled recognition. The best accuracy results were found using hispanic, race, birth year, and income. Adding gender and education decreased accuracy and increased delayed/canceled recognition. With the current structure it has almost identical performance to the Naive Bayes classifier, if slightly less good. In accuracy it still outperformed the random classifier, though did worse in delayed/canceled recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest \n",
    "With the forest classifier we had to use a different method of evaluation due to the randomness that it featured. To do this we followed the random forest procedure described in class (located at https://github.com/GonzagaCPSC322/U7-Ensemble-Learning/blob/master/A%20Ensemble%20Learning.ipynb). We computed a stratified k fold cross validation with k=3. Then we selected one of the folds as our validation set, and the rest as our training set. We trained the forest over the training set, then tested it against the validation set. This was done 5 times to attempt to minimize the effects of randomness in evaluating the classifier. We then computed the overall accuracy and confusion matrixes, and continued as normal in our evaluation. Additionally, we varied the values of N, M, and F to see if they had an effect on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Predictive Accuracy\n",
      "===========================================\n",
      "Individual Tree Accuracy:\n",
      "Tree attributes: ['a0'], validation accuracy: 0.6357069143446853\n",
      "Tree attributes: ['a0'], validation accuracy: 0.6253869969040248\n",
      "Tree attributes: ['a3'], validation accuracy: 0.6247379454926625\n",
      "Tree attributes: ['a5'], validation accuracy: 0.6219895287958115\n",
      "Tree attributes: ['a5'], validation accuracy: 0.6180698151950719\n",
      "Tree attributes: ['a4'], validation accuracy: 0.6363636363636364\n",
      "Tree attributes: ['a4'], validation accuracy: 0.6341201716738197\n",
      "Tree attributes: ['a0'], validation accuracy: 0.6260330578512396\n",
      "Tree attributes: ['a0'], validation accuracy: 0.625531914893617\n",
      "Tree attributes: ['a2'], validation accuracy: 0.6200828157349897\n",
      "Tree attributes: ['a5'], validation accuracy: 0.6227224008574491\n",
      "Tree attributes: ['a3'], validation accuracy: 0.6189967982924226\n",
      "Tree attributes: ['a2'], validation accuracy: 0.6186094069529653\n",
      "Tree attributes: ['a4'], validation accuracy: 0.6136125654450262\n",
      "Tree attributes: ['a2'], validation accuracy: 0.6129707112970711\n",
      "Tree attributes: ['a0'], validation accuracy: 0.629822732012513\n",
      "Tree attributes: ['a1'], validation accuracy: 0.6246089676746611\n",
      "Tree attributes: ['a5'], validation accuracy: 0.619979402677652\n",
      "Tree attributes: ['a5'], validation accuracy: 0.6156186612576064\n",
      "Tree attributes: ['a2'], validation accuracy: 0.6142557651991615\n",
      "Tree attributes: ['a2'], validation accuracy: 0.6240837696335079\n",
      "Tree attributes: ['a3'], validation accuracy: 0.6225026288117771\n",
      "Tree attributes: ['a3'], validation accuracy: 0.6195876288659794\n",
      "Tree attributes: ['a2'], validation accuracy: 0.6188436830835118\n",
      "Tree attributes: ['a4'], validation accuracy: 0.6176165803108808\n",
      "===========================================\n",
      "Predictive Accuracy\n",
      "===========================================\n",
      "Stratified 3-Fold Cross Validation\n",
      "Forest: accuracy = 0.614735226400614, error rate = 0.38526477359938605\n",
      "Confusion Matrices:\n",
      "==================  ==================  =============  =======  =================\n",
      "Delayed/canceled      Delayed/canceled    Not delayed    Total    Recognition (%)\n",
      "==================  ==================  =============  =======  =================\n",
      "Delayed/canceled                   157           1349     1506              10.42\n",
      "Not delayed                        179           2224     2403              92.55\n",
      "==================  ==================  =============  =======  =================\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "M = 5\n",
    "F = 1\n",
    "\n",
    "print('''===========================================\n",
    "Predictive Accuracy\n",
    "===========================================''')\n",
    "print(\"Individual Tree Accuracy:\")\n",
    "all_predicted_forest = []\n",
    "all_actual_forest = []\n",
    "# Run tests of each parameter 5 times\n",
    "for i in range(5):\n",
    "    forest = MyRandomForestClassifier(N, F, M)\n",
    "    accuracy, predicted, actual = forest.test_tree_stratified_kfold(X_train, Y_train)\n",
    "    all_predicted_forest += predicted\n",
    "    all_actual_forest += actual\n",
    "    \n",
    "    # Print off each trees individual validation acccuracy\n",
    "    for tree in forest.chosen_trees:\n",
    "        print(\"Tree attributes: \" + str(tree['attributes']) + \", validation accuracy: \" + str(tree['accuracy']))\n",
    "    \n",
    "accuracy = myutils.calculate_accuracy(all_predicted_forest, all_actual_forest)\n",
    "error_rate = 1- accuracy\n",
    "print('''===========================================\n",
    "Predictive Accuracy\n",
    "===========================================\n",
    "Stratified 3-Fold Cross Validation''')\n",
    "print(\"Forest: accuracy = \" + str(accuracy) + \", error rate = \" + str(error_rate))\n",
    "print('''Confusion Matrices:''')\n",
    "\n",
    "ylabels = list(set(Y_train))\n",
    "matrix = myevaluation.confusion_matrix(all_actual_delay_tree, all_predicted_delay_tree, ylabels)\n",
    "header = myutils.format_confusion_matrix_into_table(matrix, [\"Delayed/canceled\", \"Not delayed\"], \"Delayed/canceled\")\n",
    "\n",
    "print(tabulate(matrix, headers=header, tablefmt=\"rst\", numalign=\"right\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Predictive Accuracy\n",
      "===========================================\n",
      "Individual Tree Accuracy:\n",
      "Tree attributes: ['a2', 'a5', 'a0', 'a3'], validation accuracy: 0.6247422680412371\n",
      "Tree attributes: ['a0', 'a3', 'a2', 'a4'], validation accuracy: 0.6222684703433923\n",
      "Tree attributes: ['a3', 'a5', 'a2', 'a0'], validation accuracy: 0.6221052631578947\n",
      "Tree attributes: ['a0', 'a3', 'a4', 'a1'], validation accuracy: 0.6212278876170656\n",
      "Tree attributes: ['a0', 'a2', 'a5', 'a1'], validation accuracy: 0.6176154672395274\n",
      "Tree attributes: ['a5', 'a4', 'a2', 'a0'], validation accuracy: 0.6111111111111112\n",
      "Tree attributes: ['a0', 'a5', 'a3', 'a4'], validation accuracy: 0.6080937167199149\n",
      "Tree attributes: ['a3', 'a2', 'a4', 'a0'], validation accuracy: 0.6069489685124865\n",
      "Tree attributes: ['a2', 'a3', 'a4', 'a5'], validation accuracy: 0.6064718162839249\n",
      "Tree attributes: ['a2', 'a5', 'a1', 'a3'], validation accuracy: 0.6059670781893004\n",
      "Tree attributes: ['a4', 'a5', 'a1', 'a3'], validation accuracy: 0.6213592233009708\n",
      "Tree attributes: ['a0', 'a2', 'a5', 'a3'], validation accuracy: 0.6175514626218852\n",
      "Tree attributes: ['a3', 'a2', 'a0', 'a4'], validation accuracy: 0.6140167364016736\n",
      "Tree attributes: ['a1', 'a2', 'a0', 'a3'], validation accuracy: 0.6135163674762407\n",
      "Tree attributes: ['a0', 'a1', 'a2', 'a5'], validation accuracy: 0.6091476091476091\n",
      "Tree attributes: ['a3', 'a5', 'a1', 'a0'], validation accuracy: 0.6219895287958115\n",
      "Tree attributes: ['a1', 'a2', 'a4', 'a5'], validation accuracy: 0.6131774707757705\n",
      "Tree attributes: ['a5', 'a4', 'a1', 'a0'], validation accuracy: 0.6122233930453108\n",
      "Tree attributes: ['a1', 'a5', 'a0', 'a3'], validation accuracy: 0.6102783725910065\n",
      "Tree attributes: ['a5', 'a1', 'a2', 'a4'], validation accuracy: 0.6092077087794433\n",
      "Tree attributes: ['a4', 'a1', 'a0', 'a3'], validation accuracy: 0.6182572614107884\n",
      "Tree attributes: ['a0', 'a1', 'a4', 'a2'], validation accuracy: 0.6161719549641761\n",
      "Tree attributes: ['a3', 'a4', 'a0', 'a2'], validation accuracy: 0.6160990712074303\n",
      "Tree attributes: ['a5', 'a4', 'a2', 'a3'], validation accuracy: 0.6130389064143007\n",
      "Tree attributes: ['a4', 'a5', 'a2', 'a0'], validation accuracy: 0.6057692307692307\n",
      "===========================================\n",
      "Predictive Accuracy\n",
      "===========================================\n",
      "Stratified 3-Fold Cross Validation\n",
      "Forest: accuracy = 0.6092095165003837, error rate = 0.3907904834996163\n",
      "Confusion Matrices:\n",
      "==================  ==================  =============  =======  =================\n",
      "Delayed/canceled      Delayed/canceled    Not delayed    Total    Recognition (%)\n",
      "==================  ==================  =============  =======  =================\n",
      "Delayed/canceled                   157           1349     1506              10.42\n",
      "Not delayed                        179           2224     2403              92.55\n",
      "==================  ==================  =============  =======  =================\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "M = 5\n",
    "F = 4\n",
    "\n",
    "print('''===========================================\n",
    "Predictive Accuracy\n",
    "===========================================''')\n",
    "print(\"Individual Tree Accuracy:\")\n",
    "all_predicted_forest = []\n",
    "all_actual_forest = []\n",
    "# Run tests of each parameter 5 times\n",
    "for i in range(5):\n",
    "    forest = MyRandomForestClassifier(N, F, M)\n",
    "    accuracy, predicted, actual = forest.test_tree_stratified_kfold(X_train, Y_train)\n",
    "    all_predicted_forest += predicted\n",
    "    all_actual_forest += actual\n",
    "    \n",
    "    # Print off each trees individual validation acccuracy\n",
    "    for tree in forest.chosen_trees:\n",
    "        print(\"Tree attributes: \" + str(tree['attributes']) + \", validation accuracy: \" + str(tree['accuracy']))\n",
    "    \n",
    "accuracy = myutils.calculate_accuracy(all_predicted_forest, all_actual_forest)\n",
    "error_rate = 1- accuracy\n",
    "print('''===========================================\n",
    "Predictive Accuracy\n",
    "===========================================\n",
    "Stratified 3-Fold Cross Validation''')\n",
    "print(\"Forest: accuracy = \" + str(accuracy) + \", error rate = \" + str(error_rate))\n",
    "print('''Confusion Matrices:''')\n",
    "\n",
    "ylabels = list(set(Y_train))\n",
    "matrix = myevaluation.confusion_matrix(all_actual_delay_tree, all_predicted_delay_tree, ylabels)\n",
    "header = myutils.format_confusion_matrix_into_table(matrix, [\"Delayed/canceled\", \"Not delayed\"], \"Delayed/canceled\")\n",
    "\n",
    "print(tabulate(matrix, headers=header, tablefmt=\"rst\", numalign=\"right\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing F seems to have only decreased the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Predictive Accuracy\n",
      "===========================================\n",
      "Individual Tree Accuracy:\n",
      "Tree attributes: ['a0', 'a3'], validation accuracy: 0.6222684703433923\n",
      "Tree attributes: ['a4', 'a1'], validation accuracy: 0.6220145379023884\n",
      "Tree attributes: ['a2', 'a3'], validation accuracy: 0.6191950464396285\n",
      "Tree attributes: ['a3', 'a2'], validation accuracy: 0.6136595310907238\n",
      "Tree attributes: ['a2', 'a1'], validation accuracy: 0.6071055381400209\n",
      "Tree attributes: ['a1', 'a4'], validation accuracy: 0.6308654848800834\n",
      "Tree attributes: ['a2', 'a4'], validation accuracy: 0.6305931321540063\n",
      "Tree attributes: ['a2', 'a5'], validation accuracy: 0.6267605633802817\n",
      "Tree attributes: ['a0', 'a3'], validation accuracy: 0.6221052631578947\n",
      "Tree attributes: ['a2', 'a3'], validation accuracy: 0.6113989637305699\n",
      "Tree attributes: ['a0', 'a1'], validation accuracy: 0.6321243523316062\n",
      "Tree attributes: ['a1', 'a4'], validation accuracy: 0.6318565400843882\n",
      "Tree attributes: ['a2', 'a3'], validation accuracy: 0.6151452282157677\n",
      "Tree attributes: ['a1', 'a0'], validation accuracy: 0.6148373983739838\n",
      "Tree attributes: ['a1', 'a2'], validation accuracy: 0.6117523609653726\n",
      "Tree attributes: ['a1', 'a2'], validation accuracy: 0.6382536382536382\n",
      "Tree attributes: ['a3', 'a4'], validation accuracy: 0.6306584362139918\n",
      "Tree attributes: ['a2', 'a3'], validation accuracy: 0.628125\n",
      "Tree attributes: ['a0', 'a4'], validation accuracy: 0.6197327852004111\n",
      "Tree attributes: ['a4', 'a0'], validation accuracy: 0.6154649947753396\n",
      "Tree attributes: ['a0', 'a5'], validation accuracy: 0.6364551863041289\n",
      "Tree attributes: ['a4', 'a2'], validation accuracy: 0.635036496350365\n",
      "Tree attributes: ['a0', 'a4'], validation accuracy: 0.6302083333333334\n",
      "Tree attributes: ['a5', 'a1'], validation accuracy: 0.6254071661237784\n",
      "Tree attributes: ['a5', 'a1'], validation accuracy: 0.6252609603340292\n",
      "===========================================\n",
      "Predictive Accuracy\n",
      "===========================================\n",
      "Stratified 3-Fold Cross Validation\n",
      "Forest: accuracy = 0.614735226400614, error rate = 0.38526477359938605\n",
      "Confusion Matrices:\n",
      "==================  ==================  =============  =======  =================\n",
      "Delayed/canceled      Delayed/canceled    Not delayed    Total    Recognition (%)\n",
      "==================  ==================  =============  =======  =================\n",
      "Delayed/canceled                   157           1349     1506              10.42\n",
      "Not delayed                        179           2224     2403              92.55\n",
      "==================  ==================  =============  =======  =================\n"
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "M = 5\n",
    "F = 2\n",
    "\n",
    "print('''===========================================\n",
    "Predictive Accuracy\n",
    "===========================================''')\n",
    "print(\"Individual Tree Accuracy:\")\n",
    "all_predicted_forest = []\n",
    "all_actual_forest = []\n",
    "# Run tests of each parameter 5 times\n",
    "for i in range(5):\n",
    "    forest = MyRandomForestClassifier(N, F, M)\n",
    "    accuracy, predicted, actual = forest.test_tree_stratified_kfold(X_train, Y_train)\n",
    "    all_predicted_forest += predicted\n",
    "    all_actual_forest += actual\n",
    "    \n",
    "    # Print off each trees individual validation acccuracy\n",
    "    for tree in forest.chosen_trees:\n",
    "        print(\"Tree attributes: \" + str(tree['attributes']) + \", validation accuracy: \" + str(tree['accuracy']))\n",
    "    \n",
    "accuracy = myutils.calculate_accuracy(all_predicted_forest, all_actual_forest)\n",
    "error_rate = 1- accuracy\n",
    "print('''===========================================\n",
    "Predictive Accuracy\n",
    "===========================================\n",
    "Stratified 3-Fold Cross Validation''')\n",
    "print(\"Forest: accuracy = \" + str(accuracy) + \", error rate = \" + str(error_rate))\n",
    "print('''Confusion Matrices:''')\n",
    "\n",
    "ylabels = list(set(Y_train))\n",
    "matrix = myevaluation.confusion_matrix(all_actual_delay_tree, all_predicted_delay_tree, ylabels)\n",
    "header = myutils.format_confusion_matrix_into_table(matrix, [\"Delayed/canceled\", \"Not delayed\"], \"Delayed/canceled\")\n",
    "\n",
    "print(tabulate(matrix, headers=header, tablefmt=\"rst\", numalign=\"right\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing M seems to have marginally increased the accuracy, but not significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Predictive Accuracy\n",
      "===========================================\n",
      "Individual Tree Accuracy:\n",
      "Tree attributes: ['a3'], validation accuracy: 0.6334371754932503\n",
      "Tree attributes: ['a5'], validation accuracy: 0.6302966101694916\n",
      "Tree attributes: ['a4'], validation accuracy: 0.6446280991735537\n",
      "Tree attributes: ['a4'], validation accuracy: 0.6372950819672131\n",
      "Tree attributes: ['a4'], validation accuracy: 0.6421052631578947\n",
      "Tree attributes: ['a4'], validation accuracy: 0.6364605543710021\n",
      "Tree attributes: ['a3'], validation accuracy: 0.6469366562824507\n",
      "Tree attributes: ['a3'], validation accuracy: 0.6365568544102019\n",
      "Tree attributes: ['a4'], validation accuracy: 0.6378205128205128\n",
      "Tree attributes: ['a1'], validation accuracy: 0.6331967213114754\n",
      "===========================================\n",
      "Predictive Accuracy\n",
      "===========================================\n",
      "Stratified 3-Fold Cross Validation\n",
      "Forest: accuracy = 0.614735226400614, error rate = 0.38526477359938605\n",
      "Confusion Matrices:\n",
      "==================  ==================  =============  =======  =================\n",
      "Delayed/canceled      Delayed/canceled    Not delayed    Total    Recognition (%)\n",
      "==================  ==================  =============  =======  =================\n",
      "Delayed/canceled                   157           1349     1506              10.42\n",
      "Not delayed                        179           2224     2403              92.55\n",
      "==================  ==================  =============  =======  =================\n"
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "M = 2\n",
    "F = 1\n",
    "\n",
    "print('''===========================================\n",
    "Predictive Accuracy\n",
    "===========================================''')\n",
    "print(\"Individual Tree Accuracy:\")\n",
    "all_predicted_forest = []\n",
    "all_actual_forest = []\n",
    "# Run tests of each parameter 5 times\n",
    "for i in range(5):\n",
    "    forest = MyRandomForestClassifier(N, F, M)\n",
    "    accuracy, predicted, actual = forest.test_tree_stratified_kfold(X_train, Y_train)\n",
    "    all_predicted_forest += predicted\n",
    "    all_actual_forest += actual\n",
    "    \n",
    "    # Print off each trees individual validation acccuracy\n",
    "    for tree in forest.chosen_trees:\n",
    "        print(\"Tree attributes: \" + str(tree['attributes']) + \", validation accuracy: \" + str(tree['accuracy']))\n",
    "    \n",
    "accuracy = myutils.calculate_accuracy(all_predicted_forest, all_actual_forest)\n",
    "error_rate = 1- accuracy\n",
    "print('''===========================================\n",
    "Predictive Accuracy\n",
    "===========================================\n",
    "Stratified 3-Fold Cross Validation''')\n",
    "print(\"Forest: accuracy = \" + str(accuracy) + \", error rate = \" + str(error_rate))\n",
    "print('''Confusion Matrices:''')\n",
    "\n",
    "ylabels = list(set(Y_train))\n",
    "matrix = myevaluation.confusion_matrix(all_actual_delay_tree, all_predicted_delay_tree, ylabels)\n",
    "header = myutils.format_confusion_matrix_into_table(matrix, [\"Delayed/canceled\", \"Not delayed\"], \"Delayed/canceled\")\n",
    "\n",
    "print(tabulate(matrix, headers=header, tablefmt=\"rst\", numalign=\"right\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreasing the M value also did not noticeably modify the results.\n",
    "\n",
    "As such the best combination seems to have F=1, with M and N not having as much of an effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "The forest classifier accuracy outperformed all the other classifiers. In terms of accuracy it beat Naive Bayes, Decision Tree, and Random classifiers. It was on par with the zero R classifier, but had a better Delayed/canceled recognition rate. It didn't have as good a delayed/canceled recognition rate as random did, but it was significantly more accurate, making it still the better choice. As such it was the best. \n",
    "\n",
    "That said it was not a particularly good classifier. It only managed to be on par with the zero R classifier in terms of accuracy, and only counted as a better algorithm because its delayed/canceled recognition rate was superior to zero R's. More work would be required for it to be utilized in an important role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heroku TODOOOOO!!!!\n",
    "Here we will store the last tree we made with one of the better results.\n",
    "\n",
    "To test, type in the following url (TODO change to heroku): http://127.0.0.1:5000/predict?birth_year=1993&gender=2&hispanic=1&race=3&income=4&education=7  \n",
    "You can then modify the values and see what it predicts. Reference the introduction section where all of these are explained to determine what numbers you should place. Invalid numbers will not be accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Attribute', 'a4', ['Value', 2, ['Leaf', 2, 349, 2606]], ['Value', 4, ['Leaf', 2, 509, 2606]], ['Value', 5, ['Leaf', 2, 512, 2606]], ['Value', 1, ['Leaf', 2, 72, 2606]], ['Value', 3, ['Leaf', 2, 595, 2606]], ['Value', 6, ['Leaf', 2, 436, 2606]], ['Value', 7, ['Leaf', 2, 133, 2606]]], ['a4', 'label']], [['Attribute', 'a1', ['Value', 6, ['Leaf', 2, 445, 2606]], ['Value', 7, ['Leaf', 2, 140, 2606]], ['Value', 2, ['Leaf', 2, 343, 2606]], ['Value', 5, ['Leaf', 2, 419, 2606]], ['Value', 3, ['Leaf', 2, 612, 2606]], ['Value', 4, ['Leaf', 2, 554, 2606]], ['Value', 1, ['Leaf', 2, 93, 2606]]], ['a1', 'label']]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "trees = forest.chosen_trees\n",
    "package = []\n",
    "for tree in trees:\n",
    "    package.append([tree['tree'].tree, tree['tree'].header])\n",
    "print(package)\n",
    "pickle_path = \"forest_pickler.py\"\n",
    "outfile = open(pickle_path, \"wb\")\n",
    "pickle.dump(package, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Overall the project was moderately successful. We started with a dataset with a number of demographic and socioeconomic attributes, and whether or not individuals suffered delayed or canceled care due to COVID-19. Ounce we found the correct dataset, we were able to relatively easily use part of that data for classification. We approached our classifiers by creating two baseline ones, then testing three others against those baselines to determine how effective they were. Our best classifier, the random forest, had comparable accuracy to the first baseline classifier with better recognition rates, and outperformed the second baseline classifier by a notable amount in accuracy. However, these results were not ideal, and future development would be needed before the classifier could be useful. \n",
    "\n",
    "Future work might try predicting based on other attributes that were included in the general dataset. Employment status, other health related information, housing status, or any number of elements may have a stronger correlation. Atlernatively, the correlations of some attributes may be greater in certain areas, such as race mattering in some states more than others. Finally, it should be noted that the current classifier utilized only about 3000 of the ~70,000 data points available in one week of a study that has been going along for at least 27 weeks due to runtime issues. Utilizing more computer power and optimized algorithms to process all of the data might reveal that the current snapshot was not representative of the whole, and should definitely be investigated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
