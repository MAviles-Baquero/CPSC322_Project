{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19's Impact on Healthcare Accessibility\n",
    "By: Tristan Call and Maria Elena Aviles-Baquero  \n",
    "CPSC 322, Spring 2021  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Database:\n",
    "This section must briefly describe the dataset you used and the classification task you implemented (e.g., what were you trying to classify in the dataset).\n",
    "\n",
    "We utilized week 21 of the Household Pulse Survey Public Use File, which covered the time period from December 9 – December 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Findings:\n",
    "You should also briefly describe your findings (e.g., what classifier approach performed the best).  \n",
    "Overall we discovered that a random forest classifier was our best classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database information\n",
    "Information about the dataset itself, e.g., the attributes and attribute types, the number of instances, and the attribute being used as the label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "# uncomment once you paste your mypytable.py into mysklearn package\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.plot_utils\n",
    "importlib.reload(mysklearn.plot_utils)\n",
    "import mysklearn.plot_utils as plot_utils\n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MySimpleLinearRegressor, MyNaiveBayesClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulate Data into Useable Format\n",
    "The first thing we need to do is grab the data from the sas file and manipulate it into a format and size which is workable with our very much not optimized dataset. Part of this involves dropping rows with NaNs or -99s (seen but unanswered questions) in them ahead of time. Overall we aim to go from about 70,000 results to a more reasonable < 10,000 so that our computers can run it in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TBIRTH_YEAR  EGENDER  RHISPANIC  RRACE  EEDUC  INCOME  DELAY  NOTGET\n",
      "1          1969.0      2.0        1.0    1.0    7.0     6.0    1.0     2.0\n",
      "2          1959.0      2.0        1.0    1.0    7.0     4.0    1.0     1.0\n",
      "4          1967.0      1.0        1.0    1.0    4.0     6.0    2.0     2.0\n",
      "5          1965.0      1.0        1.0    1.0    7.0     6.0    2.0     2.0\n",
      "6          1962.0      2.0        1.0    2.0    4.0     1.0    2.0     2.0\n",
      "...           ...      ...        ...    ...    ...     ...    ...     ...\n",
      "4993       1964.0      2.0        1.0    1.0    4.0     1.0    2.0     2.0\n",
      "4994       1984.0      1.0        1.0    1.0    4.0     7.0    1.0     1.0\n",
      "4995       1973.0      1.0        1.0    1.0    6.0     8.0    2.0     2.0\n",
      "4997       1976.0      2.0        1.0    1.0    3.0     3.0    1.0     1.0\n",
      "4999       1958.0      2.0        1.0    1.0    7.0     5.0    2.0     2.0\n",
      "\n",
      "[3909 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Grab the data\n",
    "week21_filename = os.path.join(\"input_data\", \"pulse2020_puf_21.sas7bdat\")\n",
    "iterator = pd.read_sas(week21_filename, chunksize=5000)\n",
    "alldata = []\n",
    "for chunk in iterator:\n",
    "    alldata.append(chunk)\n",
    "\n",
    "relevant_attributes = [\"TBIRTH_YEAR\", \"EGENDER\", \"RHISPANIC\", \"RRACE\", \"EEDUC\", \"INCOME\", \"DELAY\", \"NOTGET\"]\n",
    "\n",
    "# Grab a chunk of data with the attributes we are interested in, minus Nans, and save to a local file\n",
    "data = alldata[0][[\"TBIRTH_YEAR\", \"EGENDER\", \"RHISPANIC\", \"RRACE\", \"EEDUC\", \"INCOME\", \"DELAY\", \"NOTGET\"]]\n",
    "working_data_filename = os.path.join(\"input_data\", \"week21_working.csv\")\n",
    "nafree_data = data.dropna()\n",
    "\n",
    "# Get rid of -99 results (aka seen but not answered)\n",
    "nafree_data = nafree_data[nafree_data.INCOME != -99]\n",
    "nafree_data = nafree_data[nafree_data.DELAY != -99]\n",
    "nafree_data = nafree_data[nafree_data.NOTGET != -99]\n",
    "print(nafree_data)\n",
    "\n",
    "# Save to file\n",
    "nafree_data.to_csv(working_data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize the data\n",
    "Next we want to get the data into a more useful format. Step one of this is chunk years into decades to have a reasonable number of attribute values for year according to the below:\n",
    "\n",
    "years | label\n",
    "-|-\n",
    "1932-1941 | 1\n",
    "1942-1951 | 2\n",
    "1952-1961 | 3\n",
    "1962-1971 | 4\n",
    "1972-1981 | 5\n",
    "1982-1991 | 6\n",
    "1992-2002 | 7\n",
    "\n",
    "Next we want to create a DELAYNOTGET column as a composite of delay and notget so we can look into both these attributes at ounce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "# Load the data into a mypytable for future analysis\n",
    "overall_table = MyPyTable()\n",
    "overall_table.load_from_file(working_data_filename)\n",
    "overall_table.convert_to_numeric()\n",
    "\n",
    "# Convert year into bigger categorical chunks\n",
    "year_col = overall_table.get_column(\"TBIRTH_YEAR\")\n",
    "year_label = [x + 1 for x in range(7)]\n",
    "cutoffs = [1932 + 10 * x for x in range(8)]\n",
    "year_col = myutils.categorize_continuous_list(year_col, cutoffs, year_label)\n",
    "\n",
    "# Create DELAYNOTGET column\n",
    "delay = overall_table.get_column(\"DELAY\")\n",
    "notget = overall_table.get_column(\"NOTGET\")\n",
    "delaynotget = []\n",
    "for i in range(len(delay)):\n",
    "    if delay[i] == 1 or notget[i] == 1:\n",
    "        delaynotget.append(1)\n",
    "    else:\n",
    "        delaynotget.append(2)\n",
    "        \n",
    "# Combine all the above into the overall_table\n",
    "overall_table.column_names.append(\"DELAYNOTGET\")\n",
    "overall_table.data = [[overall_table.data[i][0]] + [year_col[i]] + overall_table.data[i][2:] + [delaynotget[i]] for i in range(len(year_col))]\n",
    "#overall_table.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "Relevant summary statistics about the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualizations\n",
    "Data visualizations highlighting important/interesting aspects of your dataset. Visualizations may include frequency distributions, comparisons of attributes (scatterplot, multiple frequency diagrams), box and whisker plots, etc. The goal is not to include all possible diagrams, but instead to select and highlight diagrams that provide insight about the dataset itself.\n",
    "Note that this section must describe the above (in paragraph form) and not just provide diagrams and statistics. Also, each figure included must have a figure caption (Figure number and textual description) that is referenced from the text (e.g., “Figure 2 shows a frequency diagram for ...”).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "Classification Results: This section should describe the classification approach you developed and its performance. Explain what techniques you used, briefly how you designed and implemented the classifiers, how you evaluated your classifiers’ predictive ability, and how well the classifiers performed. Thoroughly describe how you evaluated performance, the comparison results, and which classifier is “best”. Include a link to a Heroku web app with this “best” classifier deployed with an API interface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Classifiers\n",
    "For this project we utilized a Naive Bayes classifier, a Decision Tree classifier, and an Ensemble tree classifier.\n",
    "\n",
    "All of these were loosley based on sklearn's DecisionTreeClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html  \n",
    "Specifically the inputs and outputs were designed so as to conform with sklearn's inputs and outputs. The internals of the algorithm were implemented by the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "To determine which classifier was the best, we ran each of them over a stratified k fold cross validation testing technique for accuracy. We then plugged these results into a confusion matrix to determine if the classifier was better at one or another prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put classifier code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Do result stuff + decide which is best  \n",
    "Add Heroku app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Conclusion: Provide a brief conclusion of your project, including a short summary of the dataset you used (and any of its inherent challenges for classification), the classification approach you developed, your classifiers performance, and any ideas you have on ways to improve its performance. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
