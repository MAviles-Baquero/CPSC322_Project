{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19's Impact on Healthcare Accessibility\n",
    "### By: Tristan Call and Maria Elena Aviles-Baquero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- load in data to use\n",
    "- create analysis functions (something that takes in specific attributes and returns them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "# uncomment once you paste your mypytable.py into mysklearn package\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.plot_utils\n",
    "importlib.reload(mysklearn.plot_utils)\n",
    "import mysklearn.plot_utils as plot_utils\n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MySimpleLinearRegressor, MyNaiveBayesClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate Data into Useable Format\n",
    "The first thing we need to do is grab the data from the sas file and manipulate it into a format and size which is workable with our very much not optimized dataset. Part of this involves dropping rows with NaNs or -99s (seen but unanswered questions) in them ahead of time. Overall we aim to go from about 70,000 results to a more reasonable < 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TBIRTH_YEAR  EGENDER  RHISPANIC  RRACE  EEDUC  INCOME  DELAY  NOTGET\n",
      "1          1969.0      2.0        1.0    1.0    7.0     6.0    1.0     2.0\n",
      "2          1959.0      2.0        1.0    1.0    7.0     4.0    1.0     1.0\n",
      "4          1967.0      1.0        1.0    1.0    4.0     6.0    2.0     2.0\n",
      "5          1965.0      1.0        1.0    1.0    7.0     6.0    2.0     2.0\n",
      "6          1962.0      2.0        1.0    2.0    4.0     1.0    2.0     2.0\n",
      "...           ...      ...        ...    ...    ...     ...    ...     ...\n",
      "4993       1964.0      2.0        1.0    1.0    4.0     1.0    2.0     2.0\n",
      "4994       1984.0      1.0        1.0    1.0    4.0     7.0    1.0     1.0\n",
      "4995       1973.0      1.0        1.0    1.0    6.0     8.0    2.0     2.0\n",
      "4997       1976.0      2.0        1.0    1.0    3.0     3.0    1.0     1.0\n",
      "4999       1958.0      2.0        1.0    1.0    7.0     5.0    2.0     2.0\n",
      "\n",
      "[3909 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Grab the data\n",
    "week21_filename = os.path.join(\"input_data\", \"pulse2020_puf_21.sas7bdat\")\n",
    "iterator = pd.read_sas(week21_filename, chunksize=5000)\n",
    "alldata = []\n",
    "for chunk in iterator:\n",
    "    alldata.append(chunk)\n",
    "\n",
    "relevant_attributes = [\"TBIRTH_YEAR\", \"EGENDER\", \"RHISPANIC\", \"RRACE\", \"EEDUC\", \"INCOME\", \"DELAY\", \"NOTGET\"]\n",
    "\n",
    "# Grab a chunk of data with the attributes we are interested in, minus Nans, and save to a local file\n",
    "data = alldata[0][[\"TBIRTH_YEAR\", \"EGENDER\", \"RHISPANIC\", \"RRACE\", \"EEDUC\", \"INCOME\", \"DELAY\", \"NOTGET\"]]\n",
    "working_data_filename = os.path.join(\"input_data\", \"week21_working.csv\")\n",
    "nafree_data = data.dropna()\n",
    "\n",
    "# Get rid of -99 results (aka seen but not answered)\n",
    "nafree_data = nafree_data[nafree_data.INCOME != -99]\n",
    "nafree_data = nafree_data[nafree_data.DELAY != -99]\n",
    "nafree_data = nafree_data[nafree_data.NOTGET != -99]\n",
    "print(nafree_data)\n",
    "\n",
    "# Save to file\n",
    "nafree_data.to_csv(working_data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize the data\n",
    "Next we want to get the data into a more useful format. Step one of this is chunk years into decades to have a reasonable number of attribute values for year according to the below:\n",
    "\n",
    "years | label\n",
    "-|-\n",
    "1932-1941 | 1\n",
    "1942-1951 | 2\n",
    "1952-1961 | 3\n",
    "1962-1971 | 4\n",
    "1972-1981 | 5\n",
    "1982-1991 | 6\n",
    "1992-2002 | 7\n",
    "\n",
    "Next we want to create a DELAYNOTGET column as a composite of delay and notget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "# Load the data into a mypytable for future analysis\n",
    "overall_table = MyPyTable()\n",
    "overall_table.load_from_file(working_data_filename)\n",
    "overall_table.convert_to_numeric()\n",
    "\n",
    "# Convert year into bigger categorical chunks\n",
    "year_col = overall_table.get_column(\"TBIRTH_YEAR\")\n",
    "year_label = [x + 1 for x in range(7)]\n",
    "cutoffs = [1932 + 10 * x for x in range(8)]\n",
    "year_col = myutils.categorize_continuous_list(year_col, cutoffs, year_label)\n",
    "\n",
    "# Create DELAYNOTGET column\n",
    "delay = overall_table.get_column(\"DELAY\")\n",
    "notget = overall_table.get_column(\"NOTGET\")\n",
    "delaynotget = []\n",
    "for i in range(len(delay)):\n",
    "    if delay[i] == 1 or notget[i] == 1:\n",
    "        delaynotget.append(1)\n",
    "    else:\n",
    "        delaynotget.append(2)\n",
    "        \n",
    "# Combine all the above into the overall_table\n",
    "overall_table.column_names.append(\"DELAYNOTGET\")\n",
    "overall_table.data = [[overall_table.data[i][0]] + [year_col[i]] + overall_table.data[i][2:] + [delaynotget[i]] for i in range(len(year_col))]\n",
    "# overall_table.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "In this section we will classify our results using our classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
